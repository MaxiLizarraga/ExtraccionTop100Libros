{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Integrador\n",
    "Realizar un Scraping de la página web de cúspide libros, www.cuspide.com , y de allí obtener el \n",
    "listado de los 100 libros más vendidos de la semana. \n",
    "* Los datos deben ser: \n",
    "1. título del libro \n",
    "2. url del libro\n",
    "3. precio en pesos\n",
    "4. precio en dólares considerando la cotización del dólar blue de Argentina o en Euros \n",
    "5. fecha. \n",
    "+ La cotización del dólar blue de Argentina o Euros debe ser obtenida también utilizando scraping de cualquier página web a elección. \n",
    "Estos datos deben ser reflejados en un archivo .csv con los respectivos campos. Asimismo debe \n",
    "crearse otro archivo de salida como log de errores, por si algún título no se puede scrapear por \n",
    "alguna razón, con la información del título del libro en cuestión, y la url del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Importamos las librerias a utilizar\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as r\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.04072451591492\n"
     ]
    }
   ],
   "source": [
    "#Probamos en solicitar la informacion a la pagina web\n",
    "requisito = r.get('https://cuspide.com/100-mas-vendidos')\n",
    "# requisito.status_code # si da 200 significa que entro satisfactoriamente a la pagina web\n",
    "\n",
    "#extraimos la informacion en general de la pagina web\n",
    "contenido_html = requisito.text\n",
    "Pagina_Principal = bs(contenido_html, 'html.parser')\n",
    "\n",
    "#extraimos las diviciones de todos los libros, cosa que nos va a ayudar a que tengamos todos los datos de cada libro y ir extrayendo lo necesario\n",
    "lista_libros = Pagina_Principal.find_all('div',class_=\"col small-6 large-2 max-w-large\")\n",
    "\n",
    "#sacamos informacion de la pagina del cronista para saber el dolar blue\n",
    "ingreso_dolar_blue = r.get('https://www.cronista.com/MercadosOnline/dolar.html')\n",
    "contenido_dolar_blue = ingreso_dolar_blue.text\n",
    "pagina_dolar_blue = bs(contenido_dolar_blue, 'html.parser')\n",
    "pagina_adentro_dolar_blue = pagina_dolar_blue.find_all('div', class_='sell-value')\n",
    "valor_dolar_blue= pagina_adentro_dolar_blue[1].text\n",
    "valor_dolar_blue= float(valor_dolar_blue.replace('$', '').replace('.', '').replace(',','.'))\n",
    "\n",
    "#guardamos el dia en el que estamos\n",
    "fecha_hoy = datetime.now().date()\n",
    "\n",
    "#creamos la variable\n",
    "nombres_libros = []\n",
    "url_libros = []\n",
    "url_libros_fallando = []\n",
    "Precio_pesos = []\n",
    "precio_dolares = []\n",
    "fecha = []\n",
    "Preciodolar_blue = []\n",
    "\n",
    "\n",
    "for nombres in lista_libros:\n",
    "        #extraemos el nombre del libro de la pagina de top100 y lo guardamos en una lista\n",
    "        nombre_libro = nombres.find('h3').text\n",
    "        nombres_libros.append(nombre_libro)\n",
    "\n",
    "        #extraemos la url\n",
    "        url = nombres.find_all('a')\n",
    "        \n",
    "        if url:\n",
    "                h_url = url[0].get('href')\n",
    "                \n",
    "        #ingresamos a la pagina del libro\n",
    "        ingreso = r.get(h_url)\n",
    "        if ingreso.status_code == 200:\n",
    "                url_libros.append(h_url)\n",
    "                contenido_html_libro = ingreso.text\n",
    "                pagina_Libro = bs(contenido_html_libro, 'html.parser')\n",
    "                pagina_Libro_adentro = pagina_Libro.find('div', class_='product-container')\n",
    "        else: url_libros_fallando.append(h_url)\n",
    "        \n",
    "        #extraemos el precio en pesos por cada libro    \n",
    "        preciopesos = pagina_Libro_adentro.find('bdi')\n",
    "        if preciopesos:\n",
    "                preciopesos = preciopesos.text\n",
    "                preciopesos = preciopesos.replace('$\\xa0', '').replace('.', '').replace(',', '.')\n",
    "                preciopesos = float(preciopesos)\n",
    "        else:\n",
    "                preciopesos = 0\n",
    "        \n",
    "        #extraemos el precio en dolares\n",
    "        preciodolar = pagina_Libro_adentro.find('span', style='font-size: 1.3em')\n",
    "        if preciodolar:\n",
    "                preciodolar = float(preciodolar.text.strip().replace(',', '.'))\n",
    "        else:\n",
    "                preciodolar = 0\n",
    "        \n",
    "        #hacemos la conversion de pesos a dolar blue\n",
    "        if preciopesos != 0:\n",
    "                precio_dolar_blue_final = round(preciopesos/valor_dolar_blue, 2)\n",
    "                Preciodolar_blue.append(precio_dolar_blue_final)\n",
    "        else: Preciodolar_blue.append(0)\n",
    "        \n",
    "        #guardamos el dia en el que se actualizó la info\n",
    "        fecha.append(fecha_hoy.strftime('%d/%m/%Y'))\n",
    "        \n",
    "        #guardamos los precios en sus respectivas listas\n",
    "        Precio_pesos.append(preciopesos)\n",
    "        precio_dolares.append(preciodolar)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = pd.DataFrame({'nombre de libros':nombres_libros , \n",
    "                      'url del libro':url_libros,\n",
    "                      'precio del libro en pesos': Precio_pesos,\n",
    "                      'precio del libro en dolares':precio_dolares,\n",
    "                       'precio dolar en dolar blue':Preciodolar_blue,\n",
    "                      'fecha': fecha\n",
    "                      })\n",
    "lista = pd.Series({'url fallados':url_libros_fallando})\n",
    "tabla.to_csv('InfoTOP100.csv',sep=',',index=False)\n",
    "lista.to_csv('logErrores.csv',sep='|',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
